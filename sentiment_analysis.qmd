---
title: "Sentiment Analysis"
author: Nicola Rennie
format:
  html:
    self-contained: true
    toc: true
    toc-location: left
    theme: litera.scss
---

# Sentiment Analysis in R

Sentiment analysis is a natural language processing technique used to analyse whether textual data is positive, negative, or neutral. A common application is the analysis of customer feedback. This tutorial serves as a brief introduction to performing sentiment analysis in R. 

## Data 

The data used here is from Week 19 of [#TidyTuesday](https://github.com/rfordatascience/tidytuesday/). The data can be downloaded from [here](https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-05-10/nyt_titles.tsv). You can either read it into R directly from the URL, or read in a locally saved copy. Here, I've saved a copy called `nyt_titles.tsv` in a directory one level up from my R script.

The data is saved as a tab-separated values (.tsv) file, and we can read it into R using the `read_tsv()` function from the {readr} package.

```{r, warning=FALSE, message=FALSE}
library(readr)
nyt_titles <- read_tsv("nyt_titles.tsv")
```

```{r, echo=FALSE}
knitr::kable(head(nyt_titles))
```

This data set contains data on books that have been on the New York Times Bestsellers List. Information is included on the title, author, the year of release, the number of weeks spent on the list, the first week on the list, the book's debut rank, and it's best rank.

For this analysis we limit ourselves to the 1,000 most popular titles, in terms of the total number of weeks spent on the list. To extract these 1,000 titles, we use `slice_max()` from {dplyr}. `slice_max()` lets you extract the `n` rows for the largest values of a particular variable. We set `with_ties = FALSE` to ensure we get exactly 1000 rows returned. For the analysis here, we will analyse the sentiment of the titles. Therefore, we extract only this column using `select()`.

```{r, warning=FALSE, message=FALSE}
library(dplyr)
text_data <- nyt_titles %>% 
  slice_max(total_weeks, n = 1000, with_ties = FALSE) %>% 
  select(title)
```

## Sentiment analysis

In R, the main package for carrying out sentiment analysis is {tidytext}. There are two main data sets we need to download. First, we load the `stop_words` data included with {tidytext}. This data set contains a list of stop words - commonly used words such as “the”, “is” or “and”. We will later use the `stop_words` data to eliminate these unimportant words. Secondly, we obtain a list of sentiments using the `get_sentiments()` function. In this example, we use the *afinn* data. This data gives a list of known words, and their sentiment score. For the afinn data, the sentiment is given as a value between -5 and 5, where -5 is negative, 0 is neutral, and 5 is positive.

```{r}
library(tidytext)
data(stop_words)
afinn_df <- get_sentiments("afinn")
```

```{r, echo=FALSE}
knitr::kable(head(afinn_df))
```

We use `unnest_tokens()` to process the titles data. This function splits each string into separate words, converts them to lowercase, and flattens the data such that each row represents a separate word. 
We then use `anti_join()` to remove the stop words - this removes any rows from the titles data for which there is a corresponding row in the `stop_words` data set. 

```{r}
text_data <- text_data %>% 
  unnest_tokens(word, title) %>% 
  anti_join(stop_words, by = "word")
```

Now, we can join the words in the titles to the words in the sentiments data set. Note that not all words in our list will have a sentiment score in the database, so we lose some data. We join the two data sets together using `inner_join()` - this keeps only words which appear in both the titles data set and the afinn sentiments data set.

```{r}
sentiments <- text_data %>% 
  inner_join(afinn_df, by = "word") 
```

```{r, echo=FALSE}
knitr::kable(head(sentiments))
```

To calculate the average sentiment across all titles, we extract the sentiment `value` column as a vector using `pull()`, and then calculate the mean. The average sentiment is 0.14 - overall slightly positive!

```{r}
sentiments %>% 
  pull(value) %>% 
  mean()
```

## Word clouds

Word clouds are commonly used to visualise the sentiments, and number of instances of words in textual data. First, we can calculate the number of times each word is used in the titles. We use `group_by()` to group together repetitions of the same word, and add the number of occurrences as a new column using `mutate()`. 

```{r}
counts <- text_data %>% 
  group_by(word) %>%  
  mutate(count = n())
```

We can then join the sentiments data set to the counts data set using `left_join()`, and we remove any duplicate rows where words are represented multiple times using `distinct()`.

```{r}
plot_data <- counts %>% 
  left_join(sentiments, by = "word") %>% 
  distinct()
```

To make our word cloud a bit nicer looking, we filter out any words that only occur once or twice. This leaves us with 104 unique words. Unfortunately, most of these don't have an associated sentiment score. If we also use `arrange()` to sort our data from most to least common words, we can see that *time* and *love* are the most common words. 

```{r}
plot_data <- plot_data %>% 
  filter(count > 2) %>% 
  arrange(desc(count))
```

```{r, echo=FALSE}
knitr::kable(head(plot_data))
```

In R, {ggplot2} is the most common data visualisation package. Unfortunately, it does not have built-in functionality to create word clouds. Therefore, we load the {ggwordcloud} package in addition. The `geom_text_wordcloud()` function is the key function that constructs the word cloud. We colour the words based on their sentiment score - green for positive words, and purple for negative words. Words that do not have a sentiment score are coloured light gray. The size of the words are given by how often they occur. The limits of the colour scale are set to match the limits of the sentiment data - between -5 and 5. 

We increase the maximum size of the words using `scale_size_area()` to 15, from the default value of 6 to make our word cloud fill the page. The `theme_void()` function removes elements such axes, gridlines, and ticks to make our wordcloud appear cleaner.

```{r, fig.align="center", width = 600, height = 400}
library(ggplot2)
library(ggwordcloud)
set.seed(42)
ggplot(data = plot_data, 
       mapping = aes(label = word, size = count, colour = value)) +
  geom_text_wordcloud() +
  scale_size_area(max_size = 15) +
  scale_colour_gradient2(low = "#710193", 
                         high = "#2e6930", 
                         na.value = "#dedede", 
                         limits = c(-5, 5)) +
  theme_void()
```

And that's it! This was a simple introduction to sentiment analysis in R. There are many more extensions to what's covered here. For this data set, I'd be really interested in looking at the changing sentiments of titles over time. You can download the data used in this analysis from [here](https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-05-10/nyt_titles.tsv), and the R script is available on [GitHub](https://github.com/nrennie/sentiment_analysis).

{{< pagebreak >}}

# Sentiment Analysis in Python

Sentiment analysis is a natural language processing technique used to analyse whether textual data is positive, negative, or neutral. A common application is the analysis of customer feedback. This tutorial serves as a brief introduction to performing sentiment analysis in R. 

## Data 

The data used here is from Week 19 of [#TidyTuesday](https://github.com/rfordatascience/tidytuesday/). The data can be downloaded from [here](https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-05-10/nyt_titles.tsv). You can either read it into Python directly from the URL, or read in a locally saved copy. Here, I've saved a copy called `nyt_titles.tsv` in a directory one level up from my Python script.

The data is saved as a tab-separated values (.tsv) file, and we can read it into Python using the `read_csv()` function from `pandas`. We also need to specify that it is a *tab* separated file, rather than a *comma* separated file.

```{python}
#| warning: false
#| message: false
import pandas as pd 
nyt_titles = pd.read_csv("nyt_titles.tsv", sep="\t") 
nyt_titles.head()
```

This data set contains data on books that have been on the New York Times Bestsellers List. Information is included on the title, author, the year of release, the number of weeks spent on the list, the first week on the list, the book's debut rank, and it's best rank.

For this analysis we limit ourselves to the 1,000 most popular titles, in terms of the total number of weeks spent on the list. To extract these 1,000 titles, we use `nlargest()`. For the sentiment analysis here, we will analyse the sentiment of the titles. Therefore we extract only the `"title"` column.

```{python}
#| warning: false
#| message: false
text_data = nyt_titles.nlargest(n=1000, columns=['total_weeks'])
text_data = text_data[["title"]] 
```

```{python}
#| echo: false
#| warning: false
#| message: false
text_data.head()
```

## Sentiment analysis

In python, there are two main libraries that we'll be using for sentiment analysis and generating word clouds. The `afinn` library gives a list of known words, and their sentiment score. For the afinn data, the sentiment is given as a value between -5 and 5, where -5 is very negative, 0 is neutral, and 5 is very positive. Secondly, the `wordcloud` package provides functionality for generating word cloud images, as well as a list of stopwords. This stopwords data set contains a list of stop words - commonly used words such as “the”, “is” or “and”. We will later use the `stop_words` data to eliminate these unimportant words.

Before the sentiment analysis is carried out, our text data requires some pre-processing. Firstly, we wish to split up the titles into individual words (using `str.split()`), and make each row in our data frame relate to each word (using `explode()`).

```{python}
#| warning: false
#| message: false
text_data["title"] = text_data["title"].str.split() 
text_data = text_data.explode("title")
```

In order to match our text data to the list of stopwords and words with sentiment scores, we also need to convert them all to lowercase using `str.lower()`, and remove any punctuation. Punctuation (and numbers) can be removed using `str.extractall()` and a regular expression. Here, we also re-format the data frame to make it a little more readable.

```{python}
#| warning: false
#| message: false
text_data["title"] = text_data["title"].str.lower()
text_data = text_data["title"].str.extractall(r"\s*([a-z@\d]+)[:\s]*")
text_data = text_data.droplevel(-1)
text_data = text_data.rename(columns={0: "word"})
text_data = text_data.reset_index()
```

```{python}
#| echo: false
#| warning: false
#| message: false
text_data.head()
```

To get the data frame of known words from the `afinn` library, we use the `Afinn()` function, and specify the language as English.

```{python}
#| warning: false
#| message: false
from afinn import Afinn
afn = Afinn(language = "en")
```

Before matching up our text data with the sentiments in the afinn data, we remove the stopwords. Here, we also import the libraries we'll need for the word clouds later, as they also come from the `wordcloud` package. 

```{python}
#| warning: false
#| message: false
from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator
stopwords = set(STOPWORDS)
text_data = text_data[~text_data.word.isin(stopwords)]
```

Now, we run through the list of words from the book titles, and match them up with their sentiment score. Words which have no match in the afinn database are given a neutral score of zero. We store the results in a data frame called `sentiments`.

```{python}
#| warning: false
#| message: false
words = tuple(text_data.word.tolist())
scores = [afn.score(word) for word in words]
sentiments = pd.DataFrame()
sentiments["word"] =  words
sentiments["scores"] = scores
```

```{python}
#| echo: false
#| warning: false
#| message: false
sentiments.head()
```
To calculate the average sentiment across all titles, we extract the import the `statistics` library and calculate the mean of the `"scores"` column. The average sentiment is 0.04 - overall only slightly more positive than neutral!

```{python}
#| warning: false
#| message: false
import statistics as st
st.mean(sentiments["scores"])
```

## Word clouds

Word clouds are commonly used to visualise the sentiments, and number of instances of words in textual data. First, we can calculate the number of times each word is used in the titles. We use the `value_counts()` function from the `numpy` library to count the number of occurrences of each word. 

```{python}
#| warning: false
#| message: false
import numpy as np
counts = pd.value_counts(np.array(words))
plot_data = pd.DataFrame()
plot_data["word"] =  counts.index
plot_data["n"] =  counts.values
```

To make our word cloud a bit nicer looking, we filter out any words that only occur once or twice.

```{python}
#| warning: false
#| message: false
plot_data = plot_data.query('n >= 3')
```

We can then join the sentiments data set to the counts data set using `merge()`, and we remove any duplicate rows where words are represented multiple times using `drop_duplicates()`.

```{python}
#| warning: false
#| message: false
plot_data = plot_data.merge(sentiments, on="word", how="left")
plot_data = plot_data.drop_duplicates()
```

```{python}
#| echo: false
#| warning: false
#| message: false
plot_data.head()
```

Finally, we can create a word cloud showing the number of occurrences of each word. We've already loaded the `wordcloud` library, but we also need to use `matplotlib` for plotting.

```{python}
#| warning: false
#| message: false
#| fig-align: center
import matplotlib.pyplot as plt
text = " ".join(i for i in text_data.word)
wordcloud = WordCloud(stopwords = stopwords, 
                      background_color = "white").generate(text)
plt.figure(figsize=(7,5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()
```

And that's it! This was a simple introduction to sentiment analysis and word clouds in Python. There are many more extensions to what's covered here. For this data set, I'd be really interested in looking at the changing sentiments of titles over time, and colouring the word cloud by sentiment rather than randomly.

You can download the data used in this analysis from [here](https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-05-10/nyt_titles.tsv), and the Python script is available on [GitHub](https://github.com/nrennie/sentiment_analysis).
